# Reddit Daily Dashboard ğŸ“Š

A beautiful dashboard to view and track the top posts from your favorite Reddit communities - daily updated with no duplicates!

**Uses the [Reddit Universal Scraper](https://github.com/ksanjeev284/reddit-universal-scraper/) - NO API KEYS REQUIRED!**

## Features

- ğŸ¨ **Beautiful Dashboard** - Modern, dark-themed UI with Streamlit
- ğŸ“… **Daily Updates** - Automatic daily scraping of top posts
- ğŸš« **No Duplicates** - Tracks seen posts to avoid showing the same content twice
- ğŸ—“ï¸ **Sort by Time** - View top monthly and yearly posts separately
- ğŸ“¸ **Screenshot Capture** - Uses ScrapeServ to take screenshots of posts
- ğŸ”„ **13 Subreddits** - Tracks posts from diverse communities
- ğŸš€ **No API Keys Needed** - Uses web scraping instead of Reddit API

## Subreddits Tracked

1. r/dataisbeautiful - Data Is Beautiful
2. r/todayilearned - Today I Learned
3. r/sobooksoc - So many books, so little time
4. r/Fitness - Fitness
5. r/getmotivated - Get Motivated!
6. r/UnethicalLifeProTips - Unethical Life Pro Tips
7. r/LifeProTips - Life Pro Tips
8. r/TrueReddit - TrueReddit
9. r/UpliftingNews - Uplifting News
10. r/lifehacks - Lifehacks
11. r/Productivity - Productivity
12. r/PersonalFinance - Personal Finance
13. r/explainlikeimfive - Explain Like I'm Five

## Prerequisites

- Python 3.8+
- Docker & Docker Compose (optional, for screenshots)
- ffmpeg (optional, for video processing)

## Quick Start

### 1. Install Dependencies

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install requirements
pip install -r requirements.txt

# Install Playwright browsers (required for scraping)
playwright install
```

### 2. Run Initial Scrape

```bash
# Scrape all subreddits (history mode - fast, no media)
python scraper_main.py dataisbeautiful --mode history --limit 20
python scraper_main.py todayilearned --mode history --limit 20
# ... repeat for other subreddits

# Or use full mode (slower, includes media)
python scraper_main.py dataisbeautiful --mode full --limit 20
```

### 3. Launch Dashboard

```bash
streamlit run dashboard.py
```

The dashboard will open at http://localhost:8501

## Daily Usage

### Manual Scraping

```bash
# Scrape a single subreddit
python scraper_main.py python --mode history --limit 50

# Full scrape with media
python scraper_main.py python --mode full --limit 100
```

### Automated Daily Scraping

Start the scheduler to automatically scrape every day:

```bash
python scheduler.py
```

This will run the scraper daily at 8:00 AM (configurable in `scheduler.py`).

### Using the Built-in Dashboard

The Reddit Universal Scraper has its own built-in dashboard:

```bash
python scraper_main.py --dashboard
```

This opens http://localhost:8501 with:
- ğŸ“Š Overview - Stats & charts
- ğŸ“ˆ Analytics - Sentiment & keywords
- ğŸ” Search - Query scraped data
- ğŸ’¬ Comments - Comment analysis
- âš™ï¸ Scraper - Start new scrapes
- ğŸ“‹ Job History - View all jobs
- ğŸ”Œ Integrations - API, export, plugins

### REST API

```bash
python scraper_main.py --api
```

Then visit http://localhost:8000/docs for API documentation.

## Project Structure

```
Reddit Scraping/
â”œâ”€â”€ config.py              # Your dashboard config
â”œâ”€â”€ database.py            # Database models
â”œâ”€â”€ dashboard.py           # Your Streamlit dashboard
â”œâ”€â”€ scheduler.py           # Daily scheduler
â”œâ”€â”€ scraper_main.py        # Reddit Universal Scraper entry point
â”œâ”€â”€ scraper/               # Scraper's core modules
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ docker-compose.yml     # Docker services
â””â”€â”€ data/                 # Data directory
    â”œâ”€â”€ r_subreddit/       # Scraped data (CSV/JSON)
    â””â”€â”€ backups/           # Database backups
```

## Docker Usage (Optional)

For screenshots with ScrapeServ:

```bash
# Start ScrapeServ only
docker compose up -d scraper

# Or start the full stack with API
docker compose up -d
```

## Troubleshooting

### Playwright issues
```bash
playwright install chromium
```

### No data showing
Make sure you've run the scraper first:
```bash
python scraper_main.py dataisbeautiful --mode history --limit 20
```

### Dashboard not loading data
Check the data folder has CSV files:
```bash
ls -la data/r_dataisbeautiful/
```

## Credits

- [Reddit Universal Scraper](https://github.com/ksanjeev284/reddit-universal-scraper/) - The scraper engine
- [ScrapeServ](https://github.com/goodreasonai/ScrapeServ) - Screenshot capture

---

Built with â¤ï¸ using Python + Streamlit
